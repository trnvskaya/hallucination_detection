{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "272def29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84517906",
   "metadata": {},
   "source": [
    "I will first define settings for this task - our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b24a6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'window_size': 2,    # will take 2 words before and 2 words after the target word\n",
    "    'n': 10,             # Embedding Dimension - one word will be represented as a vector of 10 dimensions\n",
    "    'epochs': 150,        # Number of times the whole training data will be passed through the model\n",
    "    'learning_rate': 0.01 # How much the weights will be updated during training\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ea7de3",
   "metadata": {},
   "source": [
    "I will use this text to train my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cdc4e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I would really love to get into JetBrains and learn fun ai\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1abb26",
   "metadata": {},
   "source": [
    "Now I will create a class Word2Vec to preprocess our text into a one-hot vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a4ac6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, settings):\n",
    "        self.window_size = settings['window_size']\n",
    "        self.n = settings['n']\n",
    "        self.epochs = settings['epochs']\n",
    "        self.learning_rate = settings['learning_rate']\n",
    "\n",
    "    def word2onehot(self, word):\n",
    "\n",
    "        # creating a one-hot vector in format [0, 0, 1, 0, 0,...]\n",
    "        word_vec = np.zeros(self.v_count)\n",
    "        word_index = self.word_index[word]\n",
    "        word_vec[word_index] = 1\n",
    "        return word_vec\n",
    "\n",
    "    def generate_training_data(self, text):\n",
    "\n",
    "        # counting the number of unique words in the text and creating a list of those words and their counts\n",
    "        word_counts = defaultdict(int)\n",
    "        for row in text:\n",
    "            for word in row:\n",
    "                word_counts[word] += 1\n",
    "\n",
    "\n",
    "        # creating a vocabulary of unique words and counting the number of unique words in the text\n",
    "        self.v_count = len(word_counts.keys()) #needed for matrix size\n",
    "        self.word_list = list(word_counts.keys())\n",
    "\n",
    "        # creating a dictionary that maps each word to its index and another dictionary that maps each index to its corresponding word\n",
    "        self.word_index = {word: i for i, word in enumerate(self.word_list)}\n",
    "        self.index_word = {i: word for i, word in enumerate(self.word_list)}\n",
    "\n",
    "        training_data = []\n",
    "\n",
    "        # sliding a window of size 2*window_size + 1 over the text and creating training data for each target word and its context words\n",
    "        for sentence in text:\n",
    "            sentence_len = len(sentence)\n",
    "\n",
    "            for i, word in enumerate(sentence):\n",
    "\n",
    "                # creating a one-hot vector for the target word - nn input\n",
    "                w_target = self.word2onehot(sentence[i])\n",
    "                w_context = []\n",
    "\n",
    "                for j in range(i - self.window_size, i + self.window_size + 1):\n",
    "\n",
    "                    # checking the range \n",
    "                    if j != i and j < sentence_len and j >= 0:\n",
    "                        w_context.append(self.word2onehot(sentence[j]))\n",
    "\n",
    "                # saving training data in format [target_word, [context_words]]\n",
    "                training_data.append([w_target, w_context])    \n",
    "\n",
    "        return np.array(training_data, dtype=object)    \n",
    "\n",
    "    def train(self, training_data):\n",
    "\n",
    "        # initializing weight matrices with random values between -1 and 1\n",
    "        # W1 being input -> hidden layer (V x N) and W2 being hidden -> output layer (N x V)\n",
    "\n",
    "        self.w1 = np.random.uniform(-1, 1, (self.v_count, self.n))\n",
    "        self.w2 = np.random.uniform(-1, 1, (self.n, self.v_count))\n",
    "\n",
    "        # epoch cycle\n",
    "        for epoch in range(self.epochs):\n",
    "            self.loss = 0\n",
    "\n",
    "            for w_target, w_context in training_data:\n",
    "\n",
    "                # forward pass\n",
    "\n",
    "                h = np.dot(w_target, self.w1)\n",
    "\n",
    "                u = np.dot(h, self.w2)\n",
    "\n",
    "                # applying softmax to get the predicted probabilities for each word in the vocabulary\n",
    "                y_pred = self.softmax(u)\n",
    "\n",
    "                # calculate error\n",
    "                EI = np.sum([np.subtract(y_pred, word) for word in w_context], axis=0)\n",
    "\n",
    "\n",
    "                # backpropagation\n",
    "\n",
    "                # calculating gradient for W1 \n",
    "                dl_dw1 = np.outer(w_target, np.dot(self.w2, EI.T))\n",
    "\n",
    "                # calculating gradient for W2\n",
    "                dl_dw2 = np.outer(h, EI)\n",
    "            \n",
    "                # weight update\n",
    "                self.w1 = self.w1 - (self.learning_rate * dl_dw1)\n",
    "                self.w2 = self.w2 - (self.learning_rate * dl_dw2)\n",
    "\n",
    "                # loss calculation - using negative log likelihood loss function\n",
    "                self.loss += -np.sum([u[np.argmax(word)] for word in w_context]) + len(w_context) * np.log(np.sum(np.exp(u)))\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f'Epoch: {epoch}, Loss: {self.loss}')  \n",
    "\n",
    "\n",
    "    def softmax(self, x):\n",
    "        # softmax function\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)            \n",
    "\n",
    "    def word_vec(self, word):\n",
    "\n",
    "        word_index = self.word_index[word]\n",
    "\n",
    "        # returning matrix row by index\n",
    "        return self.w1[word_index]           \n",
    "\n",
    "\n",
    "    def vec_sim(self, word, top_n):\n",
    "\n",
    "        v_w1 = self.word_vec(word)\n",
    "        word_sim = {}\n",
    "\n",
    "        for i in range(self.v_count):\n",
    "\n",
    "            v_w2 = self.w1[i]\n",
    "\n",
    "            # cosine similarity\n",
    "            theta_sum = np.dot(v_w1, v_w2)\n",
    "            theta_den = np.linalg.norm(v_w1) * np.linalg.norm(v_w2)\n",
    "            theta = theta_sum / theta_den\n",
    "\n",
    "            word = self.index_word[i]\n",
    "            word_sim[word] = theta\n",
    "\n",
    "        words_sorted = sorted(word_sim.items(), key = lambda x: x[1], reverse=True)  \n",
    "\n",
    "        for word, sim in words_sorted[:top_n]:\n",
    "            print(f\"{word}: {sim:.4f}\")           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d441b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "text= [[word.lower() for word in text.split()]]\n",
    "\n",
    "w2v = Word2Vec(settings)\n",
    "\n",
    "training_data = w2v.generate_training_data(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0aa0d0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 122.4132950252059\n",
      "Epoch: 10, Loss: 102.20198847247079\n",
      "Epoch: 20, Loss: 91.59023907394975\n",
      "Epoch: 30, Loss: 84.43484377049194\n",
      "Epoch: 40, Loss: 79.04009289790562\n",
      "Epoch: 50, Loss: 74.73456907947579\n",
      "Epoch: 60, Loss: 71.18339078869968\n",
      "Epoch: 70, Loss: 68.2030296743959\n",
      "Epoch: 80, Loss: 65.68839197780146\n",
      "Epoch: 90, Loss: 63.57364471758648\n",
      "Epoch: 100, Loss: 61.81164888346612\n",
      "Epoch: 110, Loss: 60.36366529647496\n",
      "Epoch: 120, Loss: 59.19466913725407\n",
      "Epoch: 130, Loss: 58.27015167302643\n",
      "Epoch: 140, Loss: 57.55180922819144\n"
     ]
    }
   ],
   "source": [
    "w2v.train(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e11da1",
   "metadata": {},
   "source": [
    "I will check the result of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69d7fccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for jetbrains: [-0.40147218 -0.12856303 -0.08856362 -0.05165499  0.02183594  1.4424081\n",
      " -0.7713191  -1.07088168 -1.50079747  0.06069808]\n"
     ]
    }
   ],
   "source": [
    "target_word = \"jetbrains\"\n",
    "vector = w2v.word_vec(target_word)\n",
    "\n",
    "if vector is not None:\n",
    "    print(f\"Vector for {target_word}:\", vector)\n",
    "else:\n",
    "    print(f\"Word not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2052d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jetbrains: 1.0000\n",
      "fun: 0.3253\n",
      "ai: 0.2895\n",
      "into: 0.2716\n"
     ]
    }
   ],
   "source": [
    "w2v.vec_sim(target_word, 4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
